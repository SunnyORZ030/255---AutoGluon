{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SunnyORZ030/255---AutoGluon/blob/main/colabs/03_tabular_feature_engineering_official.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4024454",
      "metadata": {
        "id": "e4024454"
      },
      "source": [
        "# AutoGluon Tabular - Feature Engineering\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/autogluon/autogluon/blob/stable/docs/tutorials/tabular/tabular-feature-engineering.ipynb)\n",
        "[![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/autogluon/autogluon/blob/stable/docs/tutorials/tabular/tabular-feature-engineering.ipynb)\n",
        "\n",
        "\n",
        "\n",
        "## Introduction ##\n",
        "\n",
        "Feature engineering involves taking raw tabular data and\n",
        "\n",
        "1. converting it into a format ready for the machine learning model to read\n",
        "2. trying to enhance some columns ('features' in ML jargon) to give the ML models more information, hoping to get more accurate results.\n",
        "\n",
        "AutoGluon does some of this for you.  This document describes how that works, and how you can extend it.  We describe here the default behaviour, much of which is configurable, as well as pointers to how to alter the behaviour from the default.\n",
        "\n",
        "## Column Types ##\n",
        "\n",
        "AutoGluon Tabular recognises the following types of features, and has separate processing for them:\n",
        "\n",
        "| Feature Type        | Example Values           |\n",
        "| :------------------ | :----------------------- |\n",
        "| boolean             | A, B                     |\n",
        "| numerical           | 1.3, 2.0, -1.6           |\n",
        "| categorical         | Red, Blue, Yellow        |\n",
        "| datetime            | 1/31/2021, Mar-31        |\n",
        "| text                | Mary had a little lamb   |\n",
        "\n",
        "In addition, other AutoGluon prediction modules recognise additional feature types, these can also be enabled in AutoGluon Tabular by using the [MultiModal](tabular-multimodal.ipynb) option.\n",
        "\n",
        "| Feature Type        | Example Values           |\n",
        "| :------------------ | :----------------------- |\n",
        "| image               | path/image123.png        |\n",
        "\n",
        "## Column Type Detection ##\n",
        "\n",
        "- Boolean columns are any columns with only 2 unique values.\n",
        "\n",
        "- Any string columns are deemed categorical unless they are text (see below).  Some models perform better if you tell them which columns are categorical and which are continuous.\n",
        "\n",
        "- Numeric columns are passed through without change, except to identify them as `float` or `int`.  Currently, numeric columns are not tested to determine if they are likely to be categorical.  You can force them to be treated as categorical with the Pandas syntax `.astype(\"category\")`, see below.\n",
        "\n",
        "- Text columns are detected by firstly checking that most rows are unique.  If they are, and there are multiple separate words detected in most rows, the row is a text column.  For details see `common/features/infer_types.py` in the source.\n",
        "\n",
        "- Datetime columns are detected by trying to convert them to Pandas datetimes.  Pandas detects a wide range of datetime formats.  If many of the values in a column are successfully converted, they are datetimes.  Currently datetimes that appear to be purely numeric (e.g. 20210530) are not correctly detected.  Any NaN values are set to the column mean.  For details see `common/features/infer_types.py`.\n",
        "\n",
        "\n",
        "## Problem Type Detection ##\n",
        "\n",
        "If the user does not specify whether the problem is a classification problem or a regression problem, the 'label' column is examined to try to guess.  Several things point towards a regression problem : the values are floating point non-integers, and there are a large amount of unique values.  Within classification, both multiclass and binary (n=2 categories) are detected.  For details see `utils/utils.py`.\n",
        "\n",
        "To override the automatic inference, explicitly pass the problem_type (one of 'binary', 'regression', 'multiclass') to `TabularPredictor()`.  For example:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e51d285",
      "metadata": {
        "id": "5e51d285"
      },
      "source": [
        "```\n",
        "predictor = TabularPredictor(label='class', problem_type='multiclass').fit(train_data)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d01b23f",
      "metadata": {
        "id": "7d01b23f"
      },
      "source": [
        "## Automatic Feature Engineering ##\n",
        "\n",
        "## Numerical Columns ##\n",
        "\n",
        "Numeric columns, both integer and floating point, currently have no automated feature engineering.\n",
        "\n",
        "## Categorical Columns ##\n",
        "\n",
        "Since many downstream models require categories to be encoded as integers, each categorical feature is mapped to monotonically increasing integers.\n",
        "\n",
        "## Datetime Columns ##\n",
        "\n",
        "Columns recognised as datetime, are converted into several features:\n",
        "\n",
        "- a numerical Pandas datetime.  Note this has maximum and minimum values specified at [pandas.Timestamp.min](https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.min.html) and [pandas.Timestamp.max](https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.min.html) respectively, which may affect extremely dates very far into the future or past.\n",
        "- several extracted columns, the default is `[year, month, day, dayofweek]`.  This is configrable via the [DatetimeFeatureGenerator](../../api/autogluon.features.rst)\n",
        "\n",
        "Note that missing, invalid and out-of-range features generated by the above logic will be converted to the mean value across all valid rows.\n",
        "\n",
        "\n",
        "## Text Columns ##\n",
        "\n",
        "If the [MultiModal](tabular-multimodal.ipynb) option is enabled, then text columns are processed using a full Transformer neural network model with pretrained NLP models.\n",
        "\n",
        "Otherwise, they are processed in two more simple ways:\n",
        "\n",
        "- an n-gram feature generator extracts n-grams (short strings) from the text feature, adding many additional columns, one for each n-gram feature.  These columns are 'n-hot' encoded, containing 1 or more if the original feature contains the n-gram 1 or more times, and 0 otherwise.  By default, all text columns are concatenated before applying this stage, and the n-grams are individual words, not substrings of words.  You can configure this via the [TextNgramFeatureGenerator](../../api/autogluon.features.rst) class. The n-gram generation is done in `generators/text_ngram.py`\n",
        "- Some additional numerical features are calculated, such as word counts, character counts, proportion of uppercase characters, etc.  This is configurable via the [TextSpecialFeatureGenerator](../../api/autogluon.features.rst).  This is done in `generators/text_special.py`\n",
        "\n",
        "## Additional Processing ##\n",
        "\n",
        "- Columns containing only 1 value are dropped before passing to models.\n",
        "- Columns containing duplicates of other columns are removed before passing to models.\n",
        "\n",
        "## Feature Engineering Example ##\n",
        "\n",
        "By default a feature generator called [AutoMLPipelineFeatureGenerator](../../api/autogluon.features.rst) is used.  Let's see this in action.  We'll create a dataframe containing a floating point column, an integer column, a datetime column,  a categorical column.  We'll first take a look at the raw data we created."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"          # 強制不用 GPU\n",
        "!{sys.executable} -m pip install -U pip setuptools wheel\n",
        "!{sys.executable} -m pip install -U autogluon.tabular autogluon.core autogluon.common"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvP2fljXsxUK",
        "outputId": "451ee53f-7a52-4976-f3d8-1c7458838ac8"
      },
      "id": "zvP2fljXsxUK",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (75.2.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (0.45.1)\n",
            "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pip-25.2 setuptools-80.9.0\n",
            "Collecting autogluon.tabular\n",
            "  Downloading autogluon.tabular-1.4.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting autogluon.core\n",
            "  Downloading autogluon.core-1.4.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting autogluon.common\n",
            "  Downloading autogluon.common-1.4.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy<2.4.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from autogluon.tabular) (2.0.2)\n",
            "Requirement already satisfied: scipy<1.17,>=1.5.4 in /usr/local/lib/python3.12/dist-packages (from autogluon.tabular) (1.16.2)\n",
            "Requirement already satisfied: pandas<2.4.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from autogluon.tabular) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn<1.8.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from autogluon.tabular) (1.6.1)\n",
            "Requirement already satisfied: networkx<4,>=3.0 in /usr/local/lib/python3.12/dist-packages (from autogluon.tabular) (3.5)\n",
            "Collecting autogluon.features==1.4.0 (from autogluon.tabular)\n",
            "  Downloading autogluon.features-1.4.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: tqdm<5,>=4.38 in /usr/local/lib/python3.12/dist-packages (from autogluon.core) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from autogluon.core) (2.32.4)\n",
            "Requirement already satisfied: matplotlib<3.11,>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from autogluon.core) (3.10.0)\n",
            "Collecting boto3<2,>=1.10 (from autogluon.core)\n",
            "  Downloading boto3-1.40.52-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: pyarrow<21.0.0,>=7.0.0 in /usr/local/lib/python3.12/dist-packages (from autogluon.common) (18.1.0)\n",
            "Requirement already satisfied: psutil<7.1.0,>=5.7.3 in /usr/local/lib/python3.12/dist-packages (from autogluon.common) (5.9.5)\n",
            "Requirement already satisfied: joblib<1.7,>=1.2 in /usr/local/lib/python3.12/dist-packages (from autogluon.common) (1.5.2)\n",
            "Collecting botocore<1.41.0,>=1.40.52 (from boto3<2,>=1.10->autogluon.core)\n",
            "  Downloading botocore-1.40.52-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2,>=1.10->autogluon.core)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3<2,>=1.10->autogluon.core)\n",
            "  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from botocore<1.41.0,>=1.40.52->boto3<2,>=1.10->autogluon.core) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<1.41.0,>=1.40.52->boto3<2,>=1.10->autogluon.core) (2.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core) (3.2.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<2.4.0,>=2.0.0->autogluon.tabular) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<2.4.0,>=2.0.0->autogluon.tabular) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.52->boto3<2,>=1.10->autogluon.core) (1.17.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.8.0,>=1.4.0->autogluon.tabular) (3.6.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->autogluon.core) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->autogluon.core) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->autogluon.core) (2025.10.5)\n",
            "Downloading autogluon.tabular-1.4.0-py3-none-any.whl (487 kB)\n",
            "Downloading autogluon.core-1.4.0-py3-none-any.whl (225 kB)\n",
            "Downloading autogluon.common-1.4.0-py3-none-any.whl (70 kB)\n",
            "Downloading autogluon.features-1.4.0-py3-none-any.whl (64 kB)\n",
            "Downloading boto3-1.40.52-py3-none-any.whl (139 kB)\n",
            "Downloading botocore-1.40.52-py3-none-any.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, autogluon.common, autogluon.features, autogluon.core, autogluon.tabular\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [autogluon.tabular]\n",
            "\u001b[1A\u001b[2KSuccessfully installed autogluon.common-1.4.0 autogluon.core-1.4.0 autogluon.features-1.4.0 autogluon.tabular-1.4.0 boto3-1.40.52 botocore-1.40.52 jmespath-1.0.1 s3transfer-0.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "\n",
        "train_url = \"https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv\"\n",
        "test_url  = \"https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv\"\n",
        "label, metric = \"class\", \"accuracy\"\n",
        "\n",
        "train_df = TabularDataset(train_url)\n",
        "test_df  = TabularDataset(test_url)\n",
        "\n",
        "# CPU 友善模型：關掉 NN_TORCH 與 AG_AUTOMM\n",
        "fast_hp = {\n",
        "    \"GBM\": {}, \"CAT\": {}, \"XGB\": {}, \"RF\": {}, \"XT\": {}, \"LR\": {},\n",
        "    \"NN_TORCH\": [], \"AG_AUTOMM\": []\n",
        "}"
      ],
      "metadata": {
        "id": "mnxrd80Bs0NI"
      },
      "id": "mnxrd80Bs0NI",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor_base = TabularPredictor(label=label, eval_metric=metric).fit(\n",
        "    train_df,\n",
        "    presets=\"medium_quality_faster_train\",\n",
        "    time_limit=120,            # 可視需要調 120~300\n",
        "    hyperparameters=fast_hp,\n",
        "    num_bag_folds=0, num_stack_levels=0,\n",
        "    ag_args_fit={\"num_gpus\": 0}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6wnQBeNs2YJ",
        "outputId": "e127850e-f41d-40b7-e633-d966fc6d9592"
      },
      "id": "y6wnQBeNs2YJ",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20251015_023424\"\n",
            "Preset alias specified: 'medium_quality_faster_train' maps to 'medium_quality'.\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.4.0\n",
            "Python Version:     3.12.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Thu Oct  2 10:42:05 UTC 2025\n",
            "CPU Count:          2\n",
            "Memory Avail:       11.39 GB / 12.67 GB (89.9%)\n",
            "Disk Space Avail:   68.18 GB / 107.72 GB (63.3%)\n",
            "===================================================\n",
            "Presets specified: ['medium_quality_faster_train']\n",
            "Beginning AutoGluon training ... Time limit = 120s\n",
            "AutoGluon will save models to \"/content/AutogluonModels/ag-20251015_023424\"\n",
            "Train Data Rows:    39073\n",
            "Train Data Columns: 14\n",
            "Label Column:       class\n",
            "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
            "\t2 unique label values:  [' <=50K', ' >50K']\n",
            "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
            "Problem Type:       binary\n",
            "Preprocessing data ...\n",
            "Selected class <--> label mapping:  class 1 =  >50K, class 0 =  <=50K\n",
            "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive ( >50K) vs negative ( <=50K) class.\n",
            "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11681.73 MB\n",
            "\tTrain Data (Original)  Memory Usage: 19.48 MB (0.2% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('int', [])    : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
            "\t\t('object', []) : 8 | ['workclass', 'education', 'marital-status', 'occupation', 'relationship', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  : 7 | ['workclass', 'education', 'marital-status', 'occupation', 'relationship', ...]\n",
            "\t\t('int', [])       : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
            "\t\t('int', ['bool']) : 1 | ['sex']\n",
            "\t0.8s = Fit runtime\n",
            "\t14 features in original data used to generate 14 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.09 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.98s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.0639828014229775, Train Rows: 36573, Val Rows: 2500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'GBM': [{}],\n",
            "\t'CAT': [{}],\n",
            "\t'XGB': [{}],\n",
            "\t'RF': [{}],\n",
            "\t'XT': [{}],\n",
            "\t'LR': [{}],\n",
            "\t'NN_TORCH': [],\n",
            "\t'AG_AUTOMM': [],\n",
            "}\n",
            "Fitting 6 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBM ... Training model for up to 119.02s of the 119.02s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.0/11.1 GB\n",
            "\t0.8824\t = Validation score   (accuracy)\n",
            "\t12.63s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: RandomForest ... Training model for up to 106.30s of the 106.30s of remaining time.\n",
            "\tFitting with cpus=2, gpus=0, mem=0.0/11.0 GB\n",
            "\t0.8612\t = Validation score   (accuracy)\n",
            "\t13.15s\t = Training   runtime\n",
            "\t0.21s\t = Validation runtime\n",
            "Fitting model: CatBoost ... Training model for up to 92.35s of the 92.34s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0\n",
            "\tWarning: Exception caused CatBoost to fail during training (ImportError)... Skipping this model.\n",
            "\t\t`import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==1.4.0`.\n",
            "Fitting model: ExtraTrees ... Training model for up to 92.13s of the 92.12s of remaining time.\n",
            "\tFitting with cpus=2, gpus=0, mem=0.0/10.9 GB\n",
            "\t0.8528\t = Validation score   (accuracy)\n",
            "\t8.67s\t = Training   runtime\n",
            "\t0.26s\t = Validation runtime\n",
            "Fitting model: XGBoost ... Training model for up to 82.31s of the 82.30s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0\n",
            "\t0.8848\t = Validation score   (accuracy)\n",
            "\t3.25s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: LinearModel ... Training model for up to 79.01s of the 79.00s of remaining time.\n",
            "\tFitting with cpus=2, gpus=0, mem=0.0/10.9 GB\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "\t0.8464\t = Validation score   (accuracy)\n",
            "\t3.8s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 119.02s of the 75.17s of remaining time.\n",
            "\tEnsemble Weights: {'XGBoost': 1.0}\n",
            "\t0.8848\t = Validation score   (accuracy)\n",
            "\t0.08s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 45.02s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 78316.8 rows/s (2500 batch size)\n",
            "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2500 rows).\n",
            "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/AutogluonModels/ag-20251015_023424\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 嘗試使用新版特徵工程管線；若環境無對應類別就退回預設\n",
        "fg = None\n",
        "try:\n",
        "    from autogluon.features import AutoMLPipelineFeatureGenerator\n",
        "    fg = AutoMLPipelineFeatureGenerator(\n",
        "        enable_categorical_features=True,\n",
        "        enable_datetime_features=True,\n",
        "        enable_text_special_features=False,   # 避免在 CPU 上變慢\n",
        "        enable_raw_text_features=False\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(\"FeatureGenerator not available, using default:\", e)\n",
        "\n",
        "predictor_fe = TabularPredictor(label=label, eval_metric=metric).fit(\n",
        "    train_df,\n",
        "    presets=\"medium_quality_faster_train\",\n",
        "    time_limit=180,            # 稍微長一點給工程處理\n",
        "    hyperparameters=fast_hp,\n",
        "    num_bag_folds=0, num_stack_levels=0,\n",
        "    ag_args_fit={\"num_gpus\": 0},\n",
        "    feature_generator=fg      # 若 fg=None，AutoGluon 會用預設自動工程\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAZjA7gjs5Px",
        "outputId": "a1e13af8-3960-40d7-a59a-e79402194e8b"
      },
      "id": "BAZjA7gjs5Px",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20251015_023823\"\n",
            "Preset alias specified: 'medium_quality_faster_train' maps to 'medium_quality'.\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.4.0\n",
            "Python Version:     3.12.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Thu Oct  2 10:42:05 UTC 2025\n",
            "CPU Count:          2\n",
            "Memory Avail:       10.65 GB / 12.67 GB (84.1%)\n",
            "Disk Space Avail:   67.55 GB / 107.72 GB (62.7%)\n",
            "===================================================\n",
            "Presets specified: ['medium_quality_faster_train']\n",
            "Beginning AutoGluon training ... Time limit = 180s\n",
            "AutoGluon will save models to \"/content/AutogluonModels/ag-20251015_023823\"\n",
            "Train Data Rows:    39073\n",
            "Train Data Columns: 14\n",
            "Label Column:       class\n",
            "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
            "\t2 unique label values:  [' <=50K', ' >50K']\n",
            "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
            "Problem Type:       binary\n",
            "Preprocessing data ...\n",
            "Selected class <--> label mapping:  class 1 =  >50K, class 0 =  <=50K\n",
            "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive ( >50K) vs negative ( <=50K) class.\n",
            "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10926.92 MB\n",
            "\tTrain Data (Original)  Memory Usage: 19.48 MB (0.2% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('int', [])    : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
            "\t\t('object', []) : 8 | ['workclass', 'education', 'marital-status', 'occupation', 'relationship', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  : 7 | ['workclass', 'education', 'marital-status', 'occupation', 'relationship', ...]\n",
            "\t\t('int', [])       : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
            "\t\t('int', ['bool']) : 1 | ['sex']\n",
            "\t0.3s = Fit runtime\n",
            "\t14 features in original data used to generate 14 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.09 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.42s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.0639828014229775, Train Rows: 36573, Val Rows: 2500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'GBM': [{}],\n",
            "\t'CAT': [{}],\n",
            "\t'XGB': [{}],\n",
            "\t'RF': [{}],\n",
            "\t'XT': [{}],\n",
            "\t'LR': [{}],\n",
            "\t'NN_TORCH': [],\n",
            "\t'AG_AUTOMM': [],\n",
            "}\n",
            "Fitting 6 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBM ... Training model for up to 179.58s of the 179.58s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.0/10.7 GB\n",
            "\t0.8824\t = Validation score   (accuracy)\n",
            "\t1.85s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: RandomForest ... Training model for up to 177.64s of the 177.64s of remaining time.\n",
            "\tFitting with cpus=2, gpus=0, mem=0.0/10.7 GB\n",
            "\t0.8612\t = Validation score   (accuracy)\n",
            "\t13.06s\t = Training   runtime\n",
            "\t0.21s\t = Validation runtime\n",
            "Fitting model: CatBoost ... Training model for up to 163.82s of the 163.81s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0\n",
            "\tWarning: Exception caused CatBoost to fail during training (ImportError)... Skipping this model.\n",
            "\t\t`import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==1.4.0`.\n",
            "Fitting model: ExtraTrees ... Training model for up to 163.59s of the 163.59s of remaining time.\n",
            "\tFitting with cpus=2, gpus=0, mem=0.0/10.7 GB\n",
            "\t0.8528\t = Validation score   (accuracy)\n",
            "\t8.84s\t = Training   runtime\n",
            "\t0.26s\t = Validation runtime\n",
            "Fitting model: XGBoost ... Training model for up to 153.60s of the 153.60s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0\n",
            "\t0.8848\t = Validation score   (accuracy)\n",
            "\t2.84s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: LinearModel ... Training model for up to 150.72s of the 150.72s of remaining time.\n",
            "\tFitting with cpus=2, gpus=0, mem=0.0/10.6 GB\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "\t0.8468\t = Validation score   (accuracy)\n",
            "\t2.97s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.58s of the 147.72s of remaining time.\n",
            "\tEnsemble Weights: {'XGBoost': 1.0}\n",
            "\t0.8848\t = Validation score   (accuracy)\n",
            "\t0.08s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 32.4s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 78138.2 rows/s (2500 batch size)\n",
            "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2500 rows).\n",
            "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/AutogluonModels/ag-20251015_023823\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json, os\n",
        "import pandas as pd\n",
        "\n",
        "RESULTS = Path(\"/content/results\"); RESULTS.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 1) Leaderboard（用「有特徵工程」的模型）\n",
        "lb = predictor_fe.leaderboard(test_df, silent=True)\n",
        "lb.to_csv(RESULTS/\"leaderboard_feature_eng_official.csv\", index=False)\n",
        "\n",
        "# 2) Metrics（高層 evaluate；只把數值轉 float）\n",
        "eval_out = predictor_fe.evaluate(test_df, silent=True)\n",
        "metrics = eval_out if isinstance(eval_out, dict) else {\"score_test\": float(eval_out)}\n",
        "metrics[\"eval_metric\"] = metric\n",
        "\n",
        "def to_jsonable(v):\n",
        "    try:\n",
        "        import numpy as np\n",
        "        if isinstance(v, (int, float, np.integer, np.floating)): return float(v)\n",
        "    except Exception: pass\n",
        "    return v\n",
        "\n",
        "with open(RESULTS/\"metrics_feature_eng_official.json\", \"w\") as f:\n",
        "    json.dump({k: to_jsonable(v) for k, v in metrics.items()}, f, indent=2)\n",
        "\n",
        "# 3)（加分）特徵重要度\n",
        "fi = predictor_fe.feature_importance(test_df, subsample_size=10000, silent=True)\n",
        "fi.reset_index().rename(columns={\"index\": \"feature\"}).to_csv(\n",
        "    RESULTS/\"feature_importance_feature_eng.csv\", index=False\n",
        ")\n",
        "\n",
        "print(\"Saved under /content/results:\")\n",
        "print(*sorted(os.listdir(RESULTS)), sep=\"\\n - \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJhtzMtis73z",
        "outputId": "be631d6b-d25c-4523-a4ac-85df1aa3075b"
      },
      "id": "RJhtzMtis73z",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved under /content/results:\n",
            "feature_importance_feature_eng.csv\n",
            " - leaderboard_feature_eng_official.csv\n",
            " - metrics_feature_eng_official.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa00faab-252f-44c9-b8f7-57131aa8251c",
      "metadata": {
        "tags": [
          "remove-cell"
        ],
        "id": "aa00faab-252f-44c9-b8f7-57131aa8251c"
      },
      "outputs": [],
      "source": [
        "!pip install autogluon.tabular[all]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d02d38de",
      "metadata": {
        "id": "d02d38de"
      },
      "outputs": [],
      "source": [
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.datasets import make_regression\n",
        "from datetime import datetime\n",
        "\n",
        "x, y = make_regression(n_samples = 100,n_features = 5,n_targets = 1, random_state = 1)\n",
        "dfx = pd.DataFrame(x, columns=['A','B','C','D','E'])\n",
        "dfy = pd.DataFrame(y, columns=['label'])\n",
        "\n",
        "# Create an integer column, a datetime column, a categorical column and a string column to demonstrate how they are processed.\n",
        "dfx['B'] = (dfx['B']).astype(int)\n",
        "dfx['C'] = datetime(2000,1,1) + pd.to_timedelta(dfx['C'].astype(int), unit='D')\n",
        "dfx['D'] = pd.cut(dfx['D'] * 10, [-np.inf,-5,0,5,np.inf],labels=['v','w','x','y'])\n",
        "dfx['E'] = pd.Series(list(' '.join(random.choice([\"abc\", \"d\", \"ef\", \"ghi\", \"jkl\"]) for i in range(4)) for j in range(100)))\n",
        "dataset=TabularDataset(dfx)\n",
        "print(dfx)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e7bd60d",
      "metadata": {
        "id": "8e7bd60d"
      },
      "source": [
        "Now let's call the default feature generator AutoMLPipeLineFeatureGenerator with no parameters and see what it does."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27804af2",
      "metadata": {
        "id": "27804af2"
      },
      "outputs": [],
      "source": [
        "from autogluon.features.generators import AutoMLPipelineFeatureGenerator\n",
        "auto_ml_pipeline_feature_generator = AutoMLPipelineFeatureGenerator()\n",
        "auto_ml_pipeline_feature_generator.fit_transform(X=dfx)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c38a293",
      "metadata": {
        "id": "2c38a293"
      },
      "source": [
        "We can see that:\n",
        "\n",
        "- The floating point and integer columns 'A' and 'B' are unchanged.\n",
        "- The datetime column 'C' has been converted to a raw value (in nanoseconds), as well as parsed into additional columns for the year, month, day and dayofweek.\n",
        "- The string categorical column 'D' has been mapped 1:1 to integers - a lot of models only accept numerical input.\n",
        "- The freeform text column has been mapped into some summary features ('char_count' etc) as well as a N-hot matrix saying whether each text contained each word.\n",
        "\n",
        "To get more details, we should call the pipeline as part of `TabularPredictor.fit()`.  We need to combine the `dfx` and `dfy` DataFrames since fit() expects a single dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79732c1f",
      "metadata": {
        "id": "79732c1f"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([dfx, dfy], axis=1)\n",
        "predictor = TabularPredictor(label='label')\n",
        "predictor.fit(df, hyperparameters={'GBM' : {}}, feature_generator=auto_ml_pipeline_feature_generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ee6addc",
      "metadata": {
        "id": "2ee6addc"
      },
      "source": [
        "Reading the output, note that:\n",
        "\n",
        "- the string-categorical column 'D', despite being mapped to integers, is still recognised as categorical.\n",
        "- the integer column 'B' has not been identified as categorical, even though it only has a few unique values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74e50c54",
      "metadata": {
        "id": "74e50c54"
      },
      "outputs": [],
      "source": [
        "print(len(set(dfx['B'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22018dfb",
      "metadata": {
        "id": "22018dfb"
      },
      "source": [
        "To mark it as categorical, we can explicitly mark it as categorical in the original dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ac7ee1f",
      "metadata": {
        "id": "2ac7ee1f"
      },
      "outputs": [],
      "source": [
        "dfx[\"B\"] = dfx[\"B\"].astype(\"category\")\n",
        "auto_ml_pipeline_feature_generator = AutoMLPipelineFeatureGenerator()\n",
        "auto_ml_pipeline_feature_generator.fit_transform(X=dfx)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59d69e45",
      "metadata": {
        "id": "59d69e45"
      },
      "source": [
        "## Missing Value Handling ##\n",
        "To illustrate missing value handling, let's set the first row to all NaNs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28bc8a4e",
      "metadata": {
        "id": "28bc8a4e"
      },
      "outputs": [],
      "source": [
        "dfx.iloc[0] = np.nan\n",
        "dfx.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac52d679",
      "metadata": {
        "id": "ac52d679"
      },
      "source": [
        "Now if we reprocess:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43d2f5dd",
      "metadata": {
        "id": "43d2f5dd"
      },
      "outputs": [],
      "source": [
        "auto_ml_pipeline_feature_generator = AutoMLPipelineFeatureGenerator()\n",
        "auto_ml_pipeline_feature_generator.fit_transform(X=dfx)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02072afe",
      "metadata": {
        "id": "02072afe"
      },
      "source": [
        "We see that the floating point, integer, categorical and text fields 'A', 'B', 'D', and 'E' have retained the NaNs, but the datetime column 'C' has been set to the mean of the non-NaN values.\n",
        "\n",
        "\n",
        "## Customization of Feature Engineering ##\n",
        "To customize your feature generation pipeline, it is recommended to call [PipelineFeatureGenerator](../../api/autogluon.features.rst), passing in non-default parameters to other feature generators as required.  For example, if we think downstream models would benefit from removing rare categorical values and replacing with NaN, we can supply the parameter maximum_num_cat to CategoryFeatureGenerator, as below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef8a9a08",
      "metadata": {
        "id": "ef8a9a08"
      },
      "outputs": [],
      "source": [
        "from autogluon.features.generators import PipelineFeatureGenerator, CategoryFeatureGenerator, IdentityFeatureGenerator\n",
        "from autogluon.common.features.types import R_INT, R_FLOAT\n",
        "mypipeline = PipelineFeatureGenerator(\n",
        "    generators = [[\n",
        "        CategoryFeatureGenerator(maximum_num_cat=10),  # Overridden from default.\n",
        "        IdentityFeatureGenerator(infer_features_in_args=dict(valid_raw_types=[R_INT, R_FLOAT])),\n",
        "    ]]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0636a109",
      "metadata": {
        "id": "0636a109"
      },
      "source": [
        "If we then dump out the transformed data, we can see that all columns have been converted to numeric, because that's what most models require, and the rare categorical values have been replaced with NaN:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab04960d",
      "metadata": {
        "id": "ab04960d"
      },
      "outputs": [],
      "source": [
        "mypipeline.fit_transform(X=dfx)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b3e9025",
      "metadata": {
        "id": "4b3e9025"
      },
      "source": [
        "For more on custom feature engineering, see the detailed notebook `examples/tabular/example_custom_feature_generator.py`."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}